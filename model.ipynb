{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "limit = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, History\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_dict(path=\"data/lemma_dict.txt\"):\n",
    "    lemma_dict = dict()\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            l = line.split()\n",
    "            lemma_dict[l[0]] = l[1]\n",
    "    return lemma_dict\n",
    "\n",
    "def get_stop_words(path=\"data/stop_words_mini.txt\"):\n",
    "    stop_words = set()\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] != '*':\n",
    "                stop_words.add(line.strip())\n",
    "    return stop_words\n",
    "\n",
    "def get_ascii(word):\n",
    "    l = \"абвгґдеєжзиіїйклмнопрстуфхцчшщьюя-\\'\"\n",
    "    s = \"!?.;\\\"'/\\\\,;()\"\n",
    "    new = \"\"\n",
    "    for w in word:\n",
    "        if w in l:\n",
    "            new += w\n",
    "        elif w in s and (new and new[-1] != ' '):\n",
    "            new += \" \"\n",
    "    return new\n",
    "\n",
    "def get_lemma_word(word, use_stop_words=True):\n",
    "    new_word = get_ascii(word.lower().strip())\n",
    "    words = [x.strip() for x in new_word.split()]\n",
    "    if len(words) <= 1:\n",
    "        if new_word and new_word in lemma_dict:\n",
    "            if not use_stop_words or new_word not in stop_words:\n",
    "                return [lemma_dict[new_word]]\n",
    "    else:\n",
    "        res = []\n",
    "        for word in words:\n",
    "            if word and word in lemma_dict:\n",
    "                if not use_stop_words or new_word not in stop_words: \n",
    "                    res.append(lemma_dict[word])\n",
    "        return res\n",
    "    return [\"\"]\n",
    "\n",
    "def get_lemma_par(par):\n",
    "    new = []\n",
    "    for sent in par.split('.'):\n",
    "        sent = get_lemma_sent(sent)\n",
    "        if sent: \n",
    "            new.append(sent)\n",
    "    return new\n",
    "\n",
    "# Main\n",
    "def get_lemma_sent(sent, use_stop_words=True):\n",
    "    new = []\n",
    "   \n",
    "    for word in sent.split():\n",
    "        word = get_lemma_word(word, use_stop_words=use_stop_words)\n",
    "        if word and word != [\"\"]: \n",
    "            for w in word:\n",
    "                new.append(w)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonyms_dict(path=\"data/antonyms.txt\"):\n",
    "    out = dict()\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            row = line.split(\",\")\n",
    "            key, value = row[0], row[1]\n",
    "            key, value = key.strip(), value.strip()\n",
    "            out[key] = value\n",
    "    return out\n",
    "                \n",
    "def process_antonyms(sent):\n",
    "    out = []\n",
    "    next_skip = False\n",
    "    for i in range(len(sent) - 1):\n",
    "        if next_skip:\n",
    "            next_skip = False\n",
    "            continue\n",
    "        if sent[i] in opposite_dict:\n",
    "            if sent[i + 1] in antonyms_dict:\n",
    "                sent[i + 1] = antonyms_dict[sent[i + 1]]\n",
    "            else:\n",
    "                next_skip = True\n",
    "        else:\n",
    "            out.append(sent[i])\n",
    "    if len(sent) < 1: return None\n",
    "    if not next_skip: out.append(sent[-1])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposite_dict = {'ні', 'не', 'без'}\n",
    "lemma_dict = get_lemma_dict()\n",
    "antonyms_dict = get_antonyms_dict()\n",
    "stop_words = get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sent(sent, use_antonyms=True, use_stop_words=True):\n",
    "    sent = get_lemma_sent(sent, use_stop_words=use_stop_words)\n",
    "    if use_antonyms:\n",
    "        sent = process_antonyms(sent)\n",
    "    if sent is None or len(sent) < 1: return None\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['сьогодні', 'тверезий']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Я була сьогодні не п'яна\"\n",
    "sent = process_sent(sent, use_antonyms=True, use_stop_words=True)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59 s, sys: 1.11 s, total: 1min\n",
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_folder = os.path.join('data', 'ubercorpus.lowercased.lemmatized.word2vec.300d')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(data_folder, binary=False)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    df = pd.read_csv(\"data/train.csv\")\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    df[\"length\"] = df[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69983, 3)\n"
     ]
    }
   ],
   "source": [
    "if train: \n",
    "    print(df.shape)\n",
    "    df = df[df[\"length\"] < limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vect(word):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_vects(data, max_len=limit, **kwargs):\n",
    "    data = process_sent(data, **kwargs)\n",
    "    out = []\n",
    "    if not data: return\n",
    "    for word in data:\n",
    "        vect = word_to_vect(word)\n",
    "        if vect is not None:\n",
    "            out.append(vect)\n",
    "    if out is None: return\n",
    "    to_add = max_len - len(out)\n",
    "    out += [[0 for _ in range(300)] for _ in range(to_add)]\n",
    "    out = np.array(out)\n",
    "    out = out.reshape(-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_vects(data, max_len=limit,**kwargs):\n",
    "    \"\"\"\n",
    "    df\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx, row in df.iterrows():\n",
    "        new_row = string_to_vects(row[\"text\"], **kwargs)\n",
    "        if new_row is not None:\n",
    "            X.append(new_row)\n",
    "            y.append([row[\"tone\"]==-1, row[\"tone\"]==0, row[\"tone\"]==1])\n",
    "    if len(X) < 1: return\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_reader(regr, message):\n",
    "    vect = string_to_vects(message)\n",
    "    return regr.predict([vect])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veyroman/.conda/envs/tests/lib/python3.5/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    df.sample(frac=1)\n",
    "    X, y = df_to_vects(df)\n",
    "    # one hot encode \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle memory\n",
    "if train: del(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = limit * 300\n",
    "hidden_shape = input_shape // 2\n",
    "out_shape = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "saver = ModelCheckpoint(\"save/run3.hdf5\", monitor='val_loss', verbose=1)\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(hidden_shape, input_dim=input_shape, kernel_initializer='normal', activation='relu'))\n",
    "classifier.add(Dropout(0.6))\n",
    "classifier.add(Dense(out_shape, kernel_initializer='normal', activation='softmax'))\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39771 samples, validate on 13257 samples\n",
      "Epoch 1/3\n",
      "39771/39771 [==============================] - 102s 3ms/step - loss: 0.6673 - acc: 0.7394 - val_loss: 0.5257 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00001: saving model to save/run2\n",
      "Epoch 2/3\n",
      "39771/39771 [==============================] - 90s 2ms/step - loss: 0.3890 - acc: 0.8344 - val_loss: 0.5611 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00002: saving model to save/run2\n",
      "Epoch 3/3\n",
      "39771/39771 [==============================] - 90s 2ms/step - loss: 0.3273 - acc: 0.8714 - val_loss: 0.6206 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00003: saving model to save/run2\n"
     ]
    }
   ],
   "source": [
    "if train: classifier.fit(X_train, y_train, epochs=3, batch_size=64, verbose=1, callbacks=[saver, history], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5892/5892 [==============================] - 2s 403us/step\n",
      "Final score: [0.64135008356398, 0.759504431538734]\n"
     ]
    }
   ],
<<<<<<< HEAD
   "source": [
    "if train: print(\"Final score:\", classifier.evaluate(X_test, y_test))"
   ]
=======
   "source": []
>>>>>>> bbe1e06b2d8b61de67a79a068deaf476dde34658
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train: print(sentence_reader(classifier, \"Це було класно\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
